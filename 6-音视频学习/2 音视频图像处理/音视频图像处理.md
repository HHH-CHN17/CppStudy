### 音视频基础

- 音视频的本质

  音频：

  1. 将人的声音通过麦克风采集转换成电信号，然后转换成数字信号，最后通过扬声器播放。

  2. 在直播项目中，我们一般是从麦克风(手机，电脑话筒)采集以后，从编程接口里面得到`pcm Buffer`，这块内存里面，就是音频数据。音频本身比较单纯，简单。编码，压缩都比较容易。
  3. 基础格式：`pcm`，`aac`，`g711`等。
  4. 单位：音频采集的采样率，比特率（可以计算出来，表示每秒钟音频数据量的大小），通道 等。像`g711`的采样率为8000，单通道；`aac` 的采样率为44100，立体声通道。
  5. 音频容器：`mp3`，`wma`，`m4a`，用于封装音频格式。

  视频：

  1. 在现实世界中，画面中，每秒刷新超过24张(帧fps)图片，就可以骗过人眼观测的范围，看起来就相当连续。
  2. 帧率：一般24 30帧，一些昂贵的先进的设备比如高端智能手机，都有60 120 240帧。
  3. 分辨率：单帧（一帧可以理解为一张图片）画面大小，1920 × 1080 1280 × 720 
  4. 码率：单张图片，数据量的大小。原始的RAW格式的，码率比较大。每一张图片可以进行压缩。

  视频采集：

  1. 从摄像头，sensor，感光元件，拍摄现实生活中的一个画面，把它存储为图片。
  2. 基础格式：`RAW`， `YUV(方便我们进行编码)`， `MJPEG`， `RGB`
  3. 常见压缩算法：`H.264`， `VP8`， `H.265`， `VP9`。在网络传输过程中，我们在保证清晰度，分辨率的情况下，尽量降低视频的数据量。数据量降低以后，就更加适合我们做网络传输。
  4. 视频容器：`Mp4`， `MKV`， `AVI`， `WMV`。包含音频，编码后的视频数据，视频的关键信息(分辨率，帧率，时间戳，时间长度等等)。

  音视频编码的原理：

  1. 进行视频编码的原因：
  2. 主要为了存储，网络传输。我们经常看到一个电影，大约2G左右，为什么这么小，主要就是因为视频进行了压缩。视频领域，过去20年 最常用的压缩算法，就是`H.264`（压缩原理大体可以理解成通过记录两帧之间的差异，实现数据的压缩，和git有点像）。超过95%的视频，都是这种压缩算法。
  3. 通过压缩算法，进行视频编码。硬件编码，iPhone，Andorid，他们都自带硬件编码芯片。硬件编码H.264/H.265性能非常强悍。甚至可以编码4K 120FPS。最差的都在200块以上。
  4. 传统的USB摄像头，用来做QQ聊天的，出于成本的考虑，没有自带H.264编码芯片，可以使用软件编码，但软件实现的效率比较低，满足低端的，普通的视频聊天使用。常见的软件编码sdk：open264，x264库（编码之后就可以使用视频容器进行存储了）。
  5. 拿到他人编码后的视频容器之后，可以通过硬件解码，软件解码（FFmpeg）使用

  音视频收集-传输全流程：

  ![image-20250318205907371](./assets/image-20250318205907371.png)

### H264视频压缩算法？？？

Qt音视频图像处理渲染引擎开发实战.pdf 52页

### 网络传输基础

- 操作系统层以上应用，95%以上的应用，都是基于两个基础的网路通讯协议，TCP/IP UDP。
- TCP/IP 简称TCP 他是一种可靠的协议。优点：在网络没故障的情况下，绝对可靠。缺点：效率低。传统的RTPM直播，4秒以上的延迟。在TCP协议栈底层，做了大量的处理，比如包的完整性校验，丢包重传机制等。这样保证每一个包收发绝对可靠。
- UDP 优点：高效率。 缺点：不可靠。
- TCP适合数据量比较小的应用，比如web应用（RTMP就是基于TCP开发的）。UDP，适合做大量数据传输，比如说，音视频。用UDP的时候，不能直接使用，要做大量的工作，比如，一定程度上的可靠性，丢包重传，完整性，网络拥塞控制等（像webrtc就是基于UDP实现的）。

---

- 目前有两种基础的应用架构

  1. 流媒体转发

     流媒体服务器：客户端采集音视频后推流到流媒体服务器，此时其他客户端可以拉流该服务器的音视频，同时由于音视频在服务器上，所以服务器可以保存该音视频以便后续进行音视频回放。

     优点：支持百万，千万级别的客户端同时观看。支持回放。只要你流媒体服务器足够多。

     缺点：资源 带宽消耗大。 

     ![image-20250318214333103](./assets/image-20250318214333103.png)

  2. P2P点对点

     使用信令服务器实现，信令服务器是用于协调通信双方（或多方）建立和管理实时音视频会话的中间服务器。它**负责交换必要的控制信息，维护一个客户端信息列表，但不直接传输音视频数据流**。

     连接上信令服务器的客户端定时向其发送本机UID对应的IP地址等信息。如果此时客户端A想和客户端B建立P2P连接，首先A需要先获取B的UID，然后通过信令服务器查询UID对应的IP地址，最后通过UDP打洞建立与B的连接

     优点：节省服务器资源，带宽。

     缺点：不能做百万级别的人同时观看。也不能做视频数据回放。

     ![image-20250318214414649](./assets/image-20250318214414649.png)

### NAT穿透

学习WebRTC的时候，P2P点对点是贯穿整个WebRTC的一个技术。受限于网络上下行带宽的限制，P2P通讯，一般2～8个人之间。由于IPV4数量限制，不得不使用网络穿透。

![image-20250319193653128](./assets/image-20250319193653128.png)

当然上图中还需要一个信令服务器

### RGB

RGB色彩模式是工业界的一种颜色标准，是通过对红(R)、绿(G)、蓝(B)三个颜色通道的变化以及它们相互之间的叠加来得到各式各样的颜色的，RGB即是代表红、绿、蓝三个通道的颜色，这个标准几乎包括了人类视力所能感知的所有颜色，是运用最广的颜色系统之一。

- RGB555 是16位的RGB格式，RGB分量都用5位表示（剩下的1位不用）。使用一个字读出一个像素后，这个字的各个位意义如下：高字节 低字节 `X R R R R R G G G G G B B B B B` （X表示不用，忽略）

- RGB565 RGB565使用16位表示一个像素，这16位中的5位用于R，6位用于G，5位用于B。程序中通常使用一个字（WORD，一个字等于两个字节）来操作一个像素。当读出一个像素后，这个字的各个位意义如下：高字节 低字节 `R R R R R G G G G G G B B B B B`

- RGB24 使用24位来表示一个像素，RGB分量都用8位表示，取值范围为0-255。注意在内存中RGB各分量的排列顺序为：`BGR BGR BGR`

- RGB32使用32位来表示一个像素，RGB分量各用去8位，剩下的8位用作Alpha通道（表示透明度）或者不用。（ARGB32就是带Alpha通道的RGB24。）注意在内存中RGB各分量的排列顺序为：`BGRA BGRA BGRA`。

习题：如果有一张1280x720大小的图片，用多大内存可以存储下这样一张RGB的图片呢？

```c++
int picSize = width * height * 3;	// RGB
//int picSize = width * height * 4;	ARGB
char* picBuff= malloc(picSize* sizeof(char));
free(picBuff);
```

### YUV？？？

[YUV格式详解【全】-CSDN博客？？？](https://blog.csdn.net/xkuzhang/article/details/115423061)

- YUV，是一种颜色编码方法。常使用在各个视频处理组件中。 YUV在对照片或视频编码时，考虑到人类的感知能力，允许降低色度的带宽。

- Y′UV,YUV,YCbCr,YPbPr所指涉的范围，常有混淆或重叠的情况。从历史的演变来说，其中YUV和Y'UV通常用来编码电视的模拟信号，而YCbCr则是用来描述数字的视频信号，适合视频与图片压缩以及传输，例如MPEG、JPEG。但在现今，YUV通常已经在电脑系统上广泛使用。

- Y'代表明亮度(luma;brightness)而U与V存储色度(色讯;chrominance;color)部分;亮度(luminance)记作Y，而Y'的prime符号记作伽玛校正。

- YUVFormats分成两个格式：

- 紧缩格式（packedformats）：将Y、U、V值存储成MacroPixels数组，和RGB的存放方式类似。

- 平面格式（planarformats）：将Y、U、V的三个分量分别存放在不同的矩阵中。

- 紧缩格式（packedformat）中的YUV是混合在一起的，对于YUV常见格式有AYUV格式（4：4：4采样、打包格式）；YUY2、UYVY（采样、打包格式），有UYVY、YUYV等。平面格式（planarformats）是指每Y分量，U分量和V分量都是以独立的平面组织的，也就是说所有的U分量必须在Y分量后面，而V分量在所有的U分量后面，此一格式适用于采样（subsample）。平面格式（planarformat）有I420（4:2:0）、YV12、IYUV等。

习题：yuv420p 也叫 i420 就是 yuv420 planar 表示。yuv420p 一共有三个平面分别是 Y，U，V，每一个平面都是用 8 bit 二进制数字表示，我们把 8 bit 称作位深度。根据前面的介绍，如果用 yuv420p 来表示分辨率为 1280 * 720 的图片，需要占用多少存储空间呢？

```c++
 // 每一个像素都需要一个 luma 值，即 y。那么总共需要 width* height。
// 每四个像素需要一个 chroma u 值，那么总共需要 width* height / 4。
// 每四个像素需要一个 chroma v 值，那么总共需要 width* height / 4
// 把 y、u、v 三个 plane 加起来就是：
double res = width * height * 6 / 4
```

### FFmpeg

- FFmpeg你可以理解为各种音视频格式编解码的一个集合，它并不是万能的

- 为什么要学习FFmpeg？绝大多数情况下，FFmpeg是一个利用CPU进行编码，解码的库。iOS macOS有AVFoundation.android有mediacodec提供更加高效的硬件编码，解码. 但是这些库，它仅仅提供了有限的格式的编解码。别的格式比如avi mkv wmv..只能用FFmpeg或其他。 所以FFmpeg是一个非常通用的，提供了绝大多数音视频格式的编解码的集合。常用的还有`Video mux/demux`功能(写mp4).

-  FFmpeg最常用的功能: 视频编码，解码. 格式转换. 写视频容器(mp4 avi…); 至于别的功能比如加水印. 服务器，推流等最好由其他库实现。

### FFMpeg 中的关键结构体？？？

[必看：FFMPEG中最关键的结构体之间的关系](https://blog.csdn.net/leixiaohua1020/article/details/11693997)

[可以看看：FFmpeg入门教程](https://blog.csdn.net/Javachichi/article/details/131660999)

注意：

1. 上面的链接中提到了“解协议，解封装，解码，存数据”，这些流程可以结合[#音视频-传输全流程](#音视频基础)进行理解。

2. 以上四个流程的详细解析：

   [ai回答，可以参考](https://yuanbao.tencent.com/bot/app/share/chat/pzejeYZn99fk)

3. 很多结构体都会涉及到一个叫做“上下文”的概念，所谓上下文，详情见下：

   [ai回答，可以参考](https://yuanbao.tencent.com/bot/app/share/chat/89T4xJ0bCwjH)

4. 复用（mux）：把不同的流按照某种容器的规则放入容器

5. 解复用(demux)：把不同的流从某种容器中解析出来

#### AVFormatContex

[必看：AVFormatContext？？？](https://blog.csdn.net/leixiaohua1020/article/details/14214705)

[选看：favformatcontext？？？](https://blog.csdn.net/qq_51282224/article/details/130990899)

- 这个结构体描述了一个媒体文件或媒体流的构成和基本信息。

  ```c++
  typedef struct AVFormatContext {
      const AVClass *av_class;   // 一个类，用于保存指向父对象的链接，用于日志记录
      struct AVInputFormat *iformat;  // 用于指定输入文件的格式以及文件读取的操作函数
      struct AVOutputFormat *oformat; // 用于指定输出文件的格式以及文件写入的操作函数
      void *priv_data;            // 指向 AVFormatContext（容器上下文）的私有数据
      AVIOContext *pb;            // 用于读取和写入媒体数据的 I/O 上下文
      int nb_streams;             // 流的数量，包括音频、视频、字幕等
      AVStream **streams;         // 指向 AVStream 结构体的指针，用于存储所有流的信息
      char *filename;             // 用于存储文件名的字符串
      int64_t start_time, duration;// 媒体文件的起始时间戳和持续时间
      int64_t bit_rate;           // 比特率，以 bit/s 计算
      uint8_t *buffer;            // 用于暂存数据的缓冲区
      int buffer_size;            // 缓冲区的大小
  } AVFormatContext;
  ```

- 这是FFMpeg中最为基本的一个结构，是其他所有结构的根，是一个多媒体文件或流的根本抽象。其中:nb_streams和streams所表示的AVStream结构指针数组包含了所有内嵌媒体流的描述；iformat和oformat指向对应的demuxer和muxer指针；pb则指向一个控制底层数据读写的ByteIOContext结构。

- 示例：

  ```c++
  #include <iostream>
  #include <string>
  #include <cstdlib>
  #include <cstdio>
  #include <cstring>
  
  // FFmpeg 头文件
  extern "C" {
  #include <libavformat/avformat.h>
  }
  
  using namespace std;
  
  int main(int argc, char* argv[]) {
      AVFormatContext* format_ctx = nullptr;
  
      // 打开待处理的视频文件
      if (avformat_open_input(&format_ctx, argv[1], nullptr, nullptr) != 0) {
          cerr << "无法打开文件" << argv[1] << endl;
          return EXIT_FAILURE;
      }
  
      // 获取流信息
      if (avformat_find_stream_info(format_ctx, nullptr) < 0) {
          cerr << "无法获取流信息" << endl;
          return EXIT_FAILURE;
      }
      
      /****************************
      经过上面两个函数，format_ctx可以得到argv[1]所指视频的信息
      ****************************/
  
      // 输出文件格式和媒体信息（即在控制台输出视频的相关信息）
      av_dump_format(format_ctx, 0, argv[1], false);
  
      // 关闭视频文件
      avformat_close_input(&format_ctx);
  
      return EXIT_SUCCESS;
  }
  ```

#### AVStream

[必看：AVStream](https://blog.csdn.net/leixiaohua1020/article/details/14215821)

- 该结构体描述一个媒体流

-  主要域的释义如下，其中大部分域的值可以由av_open_input_file根据文件头的信息确定，缺少的信息需要通过调用av_find_stream_info读帧及软解码进一步获取：

```c++
index/id 	// index对应流的索引，这个数字是自动生成的，根据index可以从AVFormatContext::streams表中索引到该流；而id则是流的标识，依赖于具体的容器格式。比如对于MPEG TS格式，id就是pid。<stream index in AVFormatContext>
time_base 	// 流的时间基准，是一个实数，该流中媒体数据的pts和dts都将以这个时间基准为粒度。通常，使用av_rescale/av_rescale_q可以实现不同时间基准的转换。
start_time 	// 流的起始时间，以流的时间基准为单位，通常是该流中第一个帧的pts。
duration 	// 流的总时间，以流的时间基准为单位。
need_parsing 	// 对该流parsing过程的控制域。
nb_frames 		// 流内的帧数目。
r_frame_rate/framerate/avg_frame_rate 	// 帧率相关。
codec 			// 指向该流对应的AVCodecContext结构，调用av_open_input_file时生成。（已被弃用，应使用parser）
parser 			// 指向该流对应的AVCodecParameters结构，调用av_find_stream_info时生成。
```

[AVCodecParameters看这里](https://blog.csdn.net/yangguoyu8023/article/details/107545229)

#### AVCodecContext

[必看：AVCodecContext-CSDN博客](https://blog.csdn.net/leixiaohua1020/article/details/14214859)

- 描述编解码器上下文的数据结构，包含了众多编解码器需要的参数信息

- 如果是单纯使用libavcodec，这部分信息需要调用者进行初始化；如果是使用整个FFMPEG库，这部分信息在调用 av_open_input_file和av_find_stream_info的过程中根据文件的头信息及媒体流内的头部信息完成初始化。其中几个主要 域的释义如下：
  -  extradata/extradata_size： 这个buffer中存放了解码器可能会用到的额外信息，在av_read_frame中填充。一般来说，首先，某种具体格式的demuxer在读取格式头 信息的时候会填充extradata，其次，如果demuxer没有做这个事情，比如可能在头部压根儿就没有相关的编解码信息，则相应的parser会继 续从已经解复用出来的媒体流中继续寻找。在没有找到任何额外信息的情况下，这个buffer指针为空。

#### AVCodec

[必看：AVCodec_ffmpeg结构体分析](https://blog.csdn.net/leixiaohua1020/article/details/14215833)

- AVCodec是存储编解码器信息的结构体

  ```c++
  const char *name 		// 编解码器的名字，比较短
  const char *long_name 	// 编解码器的名字，全称，比较长
  enum AVMediaType type 	// 指明了类型，是视频，音频，还是字幕
  enum AVCodecID id 		// ID，不重复
  const AVRational *supported_framerates 	// 支持的帧率（仅视频）
  const enum AVPixelFormat *pix_fmts 		// 支持的像素格式（仅视频）
  const int *supported_samplerates 		// 支持的采样率（仅音频）
  const enum AVSampleFormat *sample_fmts 	// 支持的采样格式（仅音频）
  const uint64_t *channel_layouts 		// 支持的声道数（仅音频）
  ```

#### 打开解码器

[必看：打开解码器](https://www.cnblogs.com/lifexy/p/13639409.html)

[必看：打开解码器](https://blog.csdn.net/qq_43257914/article/details/139878461)

1. 编码流程:
   1. 查找并分配编解码器：使用avcodec_find_encoder(codec_id)查找编解码器，然后通过avcodec_alloc_context3(codec)分配并初始化AVCodecContext。
   2. 配置编解码器上下文：设置AVCodecContext的参数，如分辨率、比特率、时间基等。
   3. 打开编解码器：调用avcodec_open2(context, codec, NULL)来初始化编解码器。
   4. 编码循环：准备数据帧，调用avcodec_send_frame送入编码器，再通过avcodec_receive_packet获取编码后的数据包。

2. 解码流程:

   1. 查找并分配解码器：与编码流程相似，使用avcodec_find_decoder(codec_id)和avcodec_alloc_context3(codec)。
   2. 配置解码器上下文：根据输入流的参数配置AVCodecContext。
   3. 打开解码器：调用avcodec_open2。
   4. 解码循环：接收数据包通过avcodec_send_packet送入解码器，然后使用avcodec_receive_frame获取解码后的帧。

3. 示例：

   ```c++
   videoStream = av_find_best_stream(ic, AVMEDIA_TYPE_VIDEO, -1, -1, NULL, 0);//获取视频流
   AVCodec *vcodec = avcodec_find_decoder(ic->streams[videoStream]->codecpar->codec_id);// 获取codec
   AVCodecContext *vc = avcodec_alloc_context3(vcodec); //构造AVCodecContext ,并将vcodec填入AVCodecContext中
   avcodec_parameters_to_context(vc, ic->streams[videoStream]->codecpar); //初始化AVCodecContext
   int ret = avcodec_open2(vc, NULL,NULL); //打开解码器,由于之前调用avcodec_alloc_context3(vcodec)初始化了vc,那么codec(第2个参数)可以填NULL
   ```

#### 复用和解复用

文件复用（Muxing）
文件复用是将多个独立的音视频流（以及其他可能的数据流，如字幕、元数据等）合并成一个单一的文件或流的过程。这个过程确保不同类型的数据可以同步传输和存储。例如，MP4、MKV、AVI等常见的多媒体文件格式都使用了复用技术来包含视频流、音频流和其他数据。

文件解复用（Demuxing）
文件解复用则是从一个复用的文件或流中分离出各个独立的音视频流和其他数据流的过程。解复用的目的是为了单独处理或播放其中的各个数据流。例如，当你在播放器中播放一个MP4文件时，播放器会解复用这个文件，以便分别解码和播放其中的音频和视频内容。

#### I帧/P帧/B帧？？？



#### 深入理解pts，dts，time_base

[深入理解pts，dts，time_base - 知乎](https://zhuanlan.zhihu.com/p/101480401)

PTS：Presentation Time Stamp。PTS主要用于度量解码后的视频帧什么时候被显示出来

DTS：[Decode Time Stamp](https://zhida.zhihu.com/search?content_id=110681903&content_type=Article&match_order=1&q=Decode+Time+Stamp&zd_token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJ6aGlkYV9zZXJ2ZXIiLCJleHAiOjE3NDI5MDI5NDYsInEiOiJEZWNvZGUgVGltZSBTdGFtcCIsInpoaWRhX3NvdXJjZSI6ImVudGl0eSIsImNvbnRlbnRfaWQiOjExMDY4MTkwMywiY29udGVudF90eXBlIjoiQXJ0aWNsZSIsIm1hdGNoX29yZGVyIjoxLCJ6ZF90b2tlbiI6bnVsbH0.Sk7ZGRbjC_DhHyRONWDeM21v4qJx42h51R8CXZ8CozU&zhida_source=entity)。DTS主要是标识读入内存中的ｂｉｔ流在什么时候开始送入解码器中进行解码

也就是pts反映帧什么时候开始显示，dts反映数据流什么时候开始解码

如果把1秒分为25等份，你可以理解就是一把尺，那么每一格表示的就是1/25秒。此时的time_base={1，25}

如果你是把1秒分成90000份，每一个刻度就是1/90000秒，此时的time_base={1，90000}。

所谓时间基表示的就是每个刻度是多少秒

pts的值就是占多少个时间刻度（占多少个格子）。它的单位不是秒，而是时间刻度。只有pts加上time_base两者同时在一起，才能表达出时间是多少。

#### AVRational

[必看：FFmpeg时间戳详解？？？](https://zhuanlan.zhihu.com/p/444774405)

AVRational 是 FFmpeg 中表示有理数（ratio）的数据结构，用于进行时间戳和帧率等计算。它的定义如下：

```c++
typedef struct AVRational{
	int num; ///< numerator
	int den; ///< denominator
} AVRational;
```

num为分子，den为分母。AVRational time_base; time_base 标示时间的刻度：（1,25）:时间刻度就是1/25（1,1000）:时间刻度就是1/1000.

AVCodecContext中的AVRational根据帧率来设定，如30帧，那么num = 1，den=30。AVStream中的time_base一般根据其采样频率设定，如（1，90000).

ffmpeg提供av_rescale_q函数用于time_base之间转换，av_rescale_q(a,b,c)作用相当于执行a*b/c，通过设置b,c的值，可以很方便的实现time_base之间转换。

编码器产生的帧放到AVStream中，那么此时packet的time要从AVCodecContext的time转换成目标AVStream的time

#### AVFrame

[AVFrame-CSDN博客](https://blog.csdn.net/leixiaohua1020/article/details/14214577)

[必看：理解AVFrame中的plane](https://yuanbao.tencent.com/bot/app/share/chat/5UFc7wgfomn8)

[必看：FFmpeg-剖析AVFrame - 知乎](https://zhuanlan.zhihu.com/p/611528191)

[必看：理解AVFrame中的stride（有点小错误，留到最后看）](https://yuanbao.tencent.com/bot/app/share/chat/26XMFqVIzbPh)

- > 在FFmpeg中，linsize[i]即stride，表示内存中一行像素数据所占的字节数；而width, height表示视频帧实际的宽高，与内存无关

- 该结构体保存的是解码后和原始的音视频信息。AVFrame通过函数av_frame_alloc()初始化，该函数仅仅分配AVFrame实例本身，而没有分配其内部的缓存。AVFrame实例由av_frame_free()释放；AVFrame实例通常分配一次，重复使用，如分配一个AVFrame实例来保留解码器中输出的视频帧（此时应在恰当的时候使用av_frame_unref()清理参考帧并将AVFrame归零）。该类所描述的数据通常由AVBuffer的API来保存一个引用计数，并保存于AVFrame.buf

- /AVFrame.extended_buf，在至少存在一个参考的时候（如AVFrame.buf[0] != NULL），则该对象被标记为“被引用”。在此情况下，AVFrame所包含的每一组数据必须包含于AVFrame的缓存中。

#### av_read_frame

- **读取AVFormatContext所指向的内容的一帧，并将该帧（包括音视频帧）放入AVPacket中**。
- av_read_frame 这个接口，它读取文件的时候，速度非常快。比如一个2G的mp4，假如它1个小时时长。一秒钟读取50M，40秒差不多就读完了。
- av_read_frame 读文件速度取决于系统I/O速度。系统I/O速度跟磁盘，驱动相关的。比如机械硬盘，每秒20～50MB之间。 固态硬盘，70～300MB。
- 所以播放器，是不能根据磁盘读取速度来播放的。要根据时间戳来播放。

#### AVPacket

[选看：AVPacket_avpacket 雷-CSDN博客](https://blog.csdn.net/leixiaohua1020/article/details/14215755)

[必看：AVPacket和其相关函数解析 - 简书](https://www.jianshu.com/p/bb6d3905907e)

[必看：AVPacket(初始化、引用、解引用、释放)_avpacket跨线程-CSDN博客](https://blog.csdn.net/ihmhm12345/article/details/115507698)

[！！！必看：ffmpeg内存模型及AVPacket和AVFrame API基本使用 - DoubleLi - 博客园](https://www.cnblogs.com/lidabo/p/15412094.html)

- AVPacket定义在avcodec.h中

- **FFMPEG使用AVPacket来暂存解复用之后、解码之前的媒体数据（也就是编码后的数据，比如h.264格式的数据）**（一个音/视频帧、一个字幕包等）及附加信息（解码时间戳、显示时间戳、时长等）。其中：

- dts 表示解码时间戳，pts表示显示时间戳，它们的单位是所属媒体流的时间基准。

- stream_index 给出所属媒体流的索引；

- data 为数据缓冲区指针，size为长度；

- duration 为数据的时长，也是以所属媒体流的时间基准为单位；

- pos 表示该数据在媒体流中的字节偏移量；

- destruct 为用于释放数据缓冲区的函数指针；

- flags 为标志域，其中，最低为置1表示该数据是一个关键帧。

### 播放器开发的过程

- 比如打开一个mp4进行播放。

- 1 FFmpeg打开这个文件，找到音频，视频的关键信息。视频的格式，分辨率。音频的格式，采样率，通道，位。用这些关键信息初始化解码器，初始化音频解码播放器。

- 2 一帧一帧去读文件。AVPacket av_read_frame

- 3 分别去解码。音频，视频解码

- 4 播放，渲染。

#### 播放器架构

下图中，Thread 1用于执行 `av_read_frame()` 函数，读取AVFormatContext中所存储的数据的一帧

![image-20250323213342594](./assets/image-20250323213342594.png)

同步问题：

通常来说，Audio Player驱动中会有一个单独的缓冲，用于保证音频播放的速度和采集速度保持一致，所以同步时应当用音频来同步视频，即使用音频的时间戳来保证视频播放速度和音频一致，来保证音视频同步。

#### SwsContext

[必看：ffmpeg-struct SwsContext使用心得-CSDN博客](https://blog.csdn.net/MACMACip/article/details/105450185)

**struct SwsContext结构体位于libswscale类库中**, 该类库主要用于处理图片像[素数](https://so.csdn.net/so/search?q=素数&spm=1001.2101.3001.7020)据, 可以完成图片像素格式的转换, 图片的拉伸等工作.

#### SwrContext

[可以了解：SwrContext重采样结构体](https://blog.csdn.net/jammg/article/details/52688506)

#### YUV字节对齐问题

- YUV 字节对齐的问题。

- FFmpeg大多数情况下，32字节对齐。

- 显示器显示图片，逐行扫描。字节对齐，跟行有关。1280x720 这样一个视频。出来如果是YUV,直接去渲染是没有问题的。因为1280能整除32. 如果是720x1280，这个时候出来的YUV，如果再按照720去拷贝行数据的话，就出问题了。因为720/32=22.5. 我们就去取23 结果linesize[0]= 32*23 = 736. 所以我们给720宽度的视频，解码出来linesize跟我们给他的不一样。

#### 音频的重新采样

当解码出来的音频采样率和音频播放器的采样率不一致时，需要重新采样
